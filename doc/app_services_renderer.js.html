<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: app/services/renderer.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: app/services/renderer.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * An angular factory service for rendering scripts using an offline audio
 * context.
 *
 * @module renderer
 * @author Creston Bunch
 */
app.factory('renderer', [function rendererFactory() {

    var NUM_CHANNELS = 2;
    var SAMPLE_RATE = 44100;

    /**
    * Render a result for offline playback.
    *
    * @param {object} result A compiled result object.
    * @returns {Promise} A promise that resolves to an AudioBuffer object.
    */
    function renderBuffer(result) {
        esconsole('Begin rendering result to buffer.', ['DEBUG','RENDERER']);

        var origin = 0;
        var duration = ESUtilsMeasureToTime(result.length+1, result.tempo); // need +1 to render to end of last measure
        var context = new (window.OfflineAudioContext ||
                      window.webkitOfflineAudioContext)
                      (NUM_CHANNELS, SAMPLE_RATE * duration, SAMPLE_RATE);

        var master = context.createGain();
        master.connect(context.destination);

        // we must go through every track and every audio clip and add each of
        // them to the audio context and start them at the right time
        // don't include the last track because we assume that's the metronome
        // track
        for (var i = 0; i &lt; result.tracks.length-1; i++) {
            var track = result.tracks[i];

            // TODO: refactor applyeffects.js
            resetAudioNodeFlags();
            var startNode = buildAudioNodeGraph(
                context, track, i, result.tempo,
                origin, master, [], 0
            );

            var trackGain = context.createGain();
            trackGain.gain.setValueAtTime(1.0, 0);

            if(i == 0) {
                if (typeof(startNode) !== "undefined") {
                    master.connect(trackGain);
                    trackGain.connect(startNode.input);
                } else {
                    master.connect(trackGain);
                    trackGain.connect(context.destination);
                }
            }

            for (var j = 0; j &lt; track.clips.length; j++) {
                var clip = track.clips[j];

                // create the audio source node to contain the audio buffer
                // and play it at the designated time
                var source = context.createBufferSource();

                // Special case for pitchshifted tracks. The pitchshifted
                // audio buffer is different than the clip audio buffer, and
                // has different start and end times
                var pitchshiftEffect =
                    track.effects['PITCHSHIFT-PITCHSHIFT_SHIFT'];
                if (pitchshiftEffect !== undefined) {
                    esconsole('Using pitchshifted audio for ' + clip.filekey +
                              ' on track ' + i,
                             ['DEBUG','RENDERER']);
                    source.buffer = clip.pitchshift.audio;
                    var start = ESUtilsMeasureToTime(
                        clip.pitchshift.start, result.tempo
                    );
                    var end = ESUtilsMeasureToTime(
                        clip.pitchshift.end, result.tempo
                    );
                // for all other tracks we can use the unprocessed clip buffer
                } else {
                    source.buffer = clip.audio;
                    var start = ESUtilsMeasureToTime(clip.start, result.tempo);
                    var end = ESUtilsMeasureToTime(clip.end, result.tempo);
                }

                // connect the buffer source to the effects tree
                if (typeof(startNode) !== "undefined")  {
                    source.connect(trackGain);
                    trackGain.connect(startNode.input)
                } else {
                    source.connect(master);
                    trackGain.connect(master);
                }

                var location = ESUtilsMeasureToTime(
                    clip.measure, result.tempo
                );

                // the clip duration may be shorter than the buffer duration
                var bufferDuration = source.buffer.duration;
                var clipDuration = end - start;

                if (origin > location &amp;&amp; origin > location + end) {
                    // case: clip is playing in the past
                    // do nothing, we don't have to play this clip

                } else if (origin > location
                           &amp;&amp; origin &lt;= location + clipDuration) {
                    // case: clip is playing from the middle
                    // calculate the offset and begin playing
                    var offset = origin - location;
                    start += offset;
                    clipDuration -= offset;
                    source.start(context.currentTime, start, clipDuration);
                    source.stop(context.currentTime + clipDuration);

                    // keep this flag so we only stop clips that are playing
                    // (otherwise we get an exception raised)
                    clip.playing = true;
                } else {
                    // case: clip is in the future
                    // calculate when it should begin and register it to play
                    var offset = location - origin;

                    source.start(
                        context.currentTime + offset, start, clipDuration
                    );
                    clip.playing = true;
                }

                // keep a reference to this audio source so we can pause it
                clip.source = source;
                clip.gain = trackGain; // used to mute the track/clip
            }
        }

        var promise = new Promise(function(resolve, reject) {
            context.startRendering();
            context.oncomplete = function(result) {
                resolve(result.renderedBuffer);

                esconsole('Render to buffer completed.', ['DEBUG','RENDERER']);
            }
        });

        return promise;
    }

    /**
    * Render a result for offline playback.
    *
    * @param {object} result A compiled result object.
    * @returns {Promise} A promise that resolves to a Blob object.
    */
    function renderWav(result) {
        return new Promise(function(resolve, reject) {
            renderBuffer(result).then(function(buffer) {
                resolve(bufferToWav(buffer));
            }).catch(function(err) {
                reject(err);
            });
        });
        }

    /**
     * Merge all the clip buffers of a track into one large buffer.
     *
     * @param {array} clips A list of compiled clips to merge.
     * @param {number} tempo The tempo to render the clips at.
     * @returns {Promise} A javascript promise that resolves to an AudioBuffer
     * of the newly rendered clip.
     */
    function mergeClips(clips, tempo) {
        esconsole('Merging clips', ['DEBUG', 'RENDERER']);
        // calculate the length of the merged clips
        var length = 0;
        for (var i = 0; i &lt; clips.length; i++) {
            var end = clips[i].measure + clips[i].end;
            if (end > length) {
                length = end;
            }
        }
        var duration = ESUtilsMeasureToTime(length, tempo);

        var promise = new Promise(function(resolve, reject) {

            // create an offline context for rendering
            var context = new (window.OfflineAudioContext ||
                          window.webkitOfflineAudioContext)
                          (NUM_CHANNELS, SAMPLE_RATE * duration, SAMPLE_RATE);

            var master = context.createGain();
            master.connect(context.destination);

            for (var i = 0; i &lt; clips.length; i++) {
                var clip = clips[i];
                var source = context.createBufferSource();
                source.buffer = clip.audio;

                source.connect(master);

                var startTime = ESUtilsMeasureToTime(clip.measure, tempo);
                var startOffset = ESUtilsMeasureToTime(clip.start, tempo);
                var endOffset = ESUtilsMeasureToTime(clip.end, tempo);

                source.start(startTime + startOffset);
                source.stop(startTime + (endOffset - startOffset));
            }

            context.startRendering();

            context.oncomplete = function(result) {
                esconsole('Merged clips', ['DEBUG','RENDERER']);
                resolve(result.renderedBuffer);
            }

        });

        return promise;
    }

    /**
     * Take a rendered offline audio context buffer and turn it into a
     * WAV file.
     *
     * @param {AudioBuffer} buffer The rendered audio buffer.
     * @returns {Blob} A blob object representing the wav file.
     */
    function bufferToWav(buffer) {
        var pcmarrayL = buffer.getChannelData(0);
        var pcmarrayR = buffer.getChannelData(1);

        var interleaved = interleave(pcmarrayL, pcmarrayR);
        var dataview = encodeWAV(interleaved);
        var audioBlob = new Blob([dataview], { type: 'audio/wav' });
        return audioBlob;
    }

    /**
     * Create an interleaved two-channel array for wave file output.
     *
     * @private
     * @param {array} inputL The left channel
     * @param {array} inputR The right channel
     * @returns {Float32Array} The interleaved array
     */
    function interleave(inputL, inputR) {
        var length = inputL.length + inputR.length;
        var result = new Float32Array(length);

        var index = 0,
            inputIndex = 0;

        while (index &lt; length) {
            result[index++] = inputL[inputIndex];
            result[index++] = inputR[inputIndex];
            inputIndex++;
        }
        return result;
    }

    /**
     * Encode an array of interleaved 2-channel samples to a wav file.
     *
     * @private
     * @param {array} samples The interleaved array samples.
     * @return {DataView}
     */
    function encodeWAV(samples) {
        var buffer = new ArrayBuffer(44 + samples.length * 2);
        var view = new DataView(buffer);

        /* RIFF identifier */
        writeString(view, 0, 'RIFF');
        /* file length */
        view.setUint32(4, 32 + samples.length * 2, true);
        /* RIFF type */
        writeString(view, 8, 'WAVE');
        /* format chunk identifier */
        writeString(view, 12, 'fmt ');
        /* format chunk length */
        view.setUint32(16, 16, true);
        /* sample format (raw) */
        view.setUint16(20, 1, true);
        /* channel count */
        view.setUint16(22, 2, true);
        /* sample rate */
        view.setUint32(24, SAMPLE_RATE, true);
        /* byte rate (sample rate * block align) */
        view.setUint32(28, SAMPLE_RATE * 4, true);
        /* block align (channel count * bytes per sample) */
        view.setUint16(32, 4, true);
        /* bits per sample */
        view.setUint16(34, 16, true);
        /* data chunk identifier */
        writeString(view, 36, 'data');
        /* data chunk length */
        view.setUint32(40, samples.length * 2, true);

        floatTo16BitPCM(view, 44, samples);

        return view;
    }

    /**
     * Convert a float array to 16 bit PCM
     *
     * @private
     */
    function floatTo16BitPCM(output, offset, input) {
        for (var i = 0; i &lt; input.length; i++, offset += 2) {
            var s = Math.max(-1, Math.min(1, input[i]));
            output.setInt16(offset, s &lt; 0 ? s * 0x8000 : s * 0x7FFF, true);
        }
    }

    /*
     * @private
     */
    function writeString(view, offset, string) {
        for (var i = 0; i &lt; string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
        }
    }

    return {
        renderBuffer: renderBuffer,
        renderWav: renderWav,
        mergeClips: mergeClips
    };

}]);

</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Modules</h3><ul><li><a href="module-accountController.html">accountController</a></li><li><a href="module-Analyze.html">Analyze</a></li><li><a href="module-apibrowserController.html">apibrowserController</a></li><li><a href="module-audioContext.html">audioContext</a></li><li><a href="module-audioeffects.html">audioeffects</a></li><li><a href="module-audioLibrary.html">audioLibrary</a></li><li><a href="module-autograderController.html">autograderController</a></li><li><a href="module-buildAudioNodeGraph.html">buildAudioNodeGraph</a></li><li><a href="module-changepasswordController.html">changepasswordController</a></li><li><a href="module-ColorPickerCtrl.html">ColorPickerCtrl</a></li><li><a href="module-compiler.html">compiler</a></li><li><a href="module-dawClip.html">dawClip</a></li><li><a href="module-dawClipName.html">dawClipName</a></li><li><a href="module-dawContainerController.html">dawContainerController</a></li><li><a href="module-dawController.html">dawController</a></li><li><a href="module-dawEffect.html">dawEffect</a></li><li><a href="module-dawTimeline.html">dawTimeline</a></li><li><a href="module-dawTrackController.html">dawTrackController</a></li><li><a href="module-DownloadFileCtrl.html">DownloadFileCtrl</a></li><li><a href="module-EarSketchApp.html">EarSketchApp</a></li><li><a href="module-forgotpasswordController.html">forgotpasswordController</a></li><li><a href="module-ideController.html">ideController</a></li><li><a href="module-JavascriptAPI.html">JavascriptAPI</a></li><li><a href="module-mainController.html">mainController</a></li><li><a href="module-NativeJavascriptAPI.html">NativeJavascriptAPI</a></li><li><a href="module-Passthrough.html">Passthrough</a></li><li><a href="module-pitchshifter.html">pitchshifter</a></li><li><a href="module-player.html">player</a></li><li><a href="module-PythonAPI.html">PythonAPI</a></li><li><a href="module-renameController.html">renameController</a></li><li><a href="module-renderer.html">renderer</a></li><li><a href="module-ReportErrorCtrl.html">ReportErrorCtrl</a></li><li><a href="module-SaveScriptCtrl.html">SaveScriptCtrl</a></li><li><a href="module-uploader.html">uploader</a></li><li><a href="module-userConsole.html">userConsole</a></li><li><a href="module-userNotification.html">userNotification</a></li><li><a href="module-WaveforrmCache.html">WaveforrmCache</a></li></ul><h3>Global</h3><ul><li><a href="global.html#addESTagToLog">addESTagToLog</a></li><li><a href="global.html#addESTagToNotLog">addESTagToNotLog</a></li><li><a href="global.html#addESTagToNotPrint">addESTagToNotPrint</a></li><li><a href="global.html#addESTagToPrint">addESTagToPrint</a></li><li><a href="global.html#esconsole">esconsole</a></li><li><a href="global.html#ESLog">ESLog</a></li><li><a href="global.html#formatResultForTests">formatResultForTests</a></li><li><a href="global.html#formatScriptForTests">formatScriptForTests</a></li><li><a href="global.html#getURLParameters">getURLParameters</a></li><li><a href="global.html#ServiceWrapper">ServiceWrapper</a></li><li><a href="global.html#setESConsoleTraceLevel">setESConsoleTraceLevel</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 3.3.3</a> on Thu Feb 11 2016 16:06:08 GMT-0500 (EST)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
